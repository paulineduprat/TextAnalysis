{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Investigating Linguistic \n",
    "# Differences Between Cities \n",
    "Pauline Duprat and Landon Kleinbrodt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Packages\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML, display\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from math import radians, cos, sin, asin, sqrt, floor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous Studies\n",
    "\n",
    "* Focused on specific, colloquial phrases\n",
    "    * Joshua Katz: Speaking American\n",
    "\n",
    "* Compared two or three close cities, often with a survey of relatively few people\n",
    "    * Kaitlyn Lee: Analysis of Regional Linguistic Variation\n",
    "\n",
    "* Phonetics, pronunciations, accents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Previous studies into regional linguistics have generally focused on one of three themes. First, they focus on studying specific, often colloquial, phrases. An example of this study would be Joshua Katz' book \"Speaking American\" where the titular image of this notebook was found. This research would analyze how different regions referred to the same objects or ideas. It would often focus on slang and would not delve at all into every day language.\n",
    "\n",
    "Another genre of study would be the close examination of one or two dialects. Kaitlyn Lee did such a study in her \"Analysis of Regional Linguistic Variation\" where she closely examined two neighboring towns that lay across state lines. These studies are often conducted using survey methods and have a very specific scope and focus. Such study attempts to establish patterns of speech across a very small geospatial divide.\n",
    "\n",
    "Finally, considerable study has been done into phonetics, pronunciations, and accents. These studies are sound-based and care little about the actual vocabulary used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## This study:\n",
    "\n",
    "* Computational Methods\n",
    "    * Data driven rather than hand driven\n",
    "    * Allows construction of linguistic identity rather than assignment of one\n",
    "    * Can study cities far away from each other\n",
    "\n",
    "* Not focused on speech, but vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Not studying how different regions refer to the same object, but rather how different regions speak about satisfaction\n",
    "    * *Not*: \"What would you call the drink this store sells?\"\n",
    "    * *Rather*: \"How do different cities talk about stores differently?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our study differs from the rest in a few ways, and most of these differences are based in our computational methods. First, it does not focus on specific phrases or words. It does not look to establish synonyms found across cultures. Rather, we wish to have a more holistic view of a regions vernacular. Having a data driven approach means that we will not be providing our algorithims with specific words or phrases to look at; rather we will be guiding models so that they can tell *us* about the vocabulary of different regions. In this way it is different from studies like Katz' and Lee's. Our methods will construct a linguistic identity through the study of text, rather than using establish textual patterns to analyze a linguistic identity. Furthermore, using computational methods allows us to study cities thousands of miles apart from eachother. It will also allow us to possibly establish a gradient of language, whereas previous survey models would be too discrete to establish such trends. Finally, our study is focused entirely on text, and thus questions of pronunciations or speech are not involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./Data/my_reviews.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c1a58ad8b80d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Data/my_reviews.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'funny'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cool'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'useful'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreviews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Data/reviews.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbusiness_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Data/business.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mp_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"business_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"city\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stars_y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#stars_y because this is each individual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pauline/anaconda/envs/hist100/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pauline/anaconda/envs/hist100/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pauline/anaconda/envs/hist100/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pauline/anaconda/envs/hist100/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pauline/anaconda/envs/hist100/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./Data/my_reviews.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Data/my_reviews.csv\", encoding = \"utf-8\" , dtype = {'stars':float,'funny':float, 'cool':float, 'useful':float})\n",
    "reviews_df = pd.read_csv(\"./Data/reviews.csv\", sep = ',', encoding = \"utf-8\")\n",
    "business_df = pd.read_csv(\"./Data/business.csv\", sep = ',', encoding = \"utf-8\")\n",
    "p_reviews = pd.merge(business_df, reviews_df, on = \"business_id\")\n",
    "p_reviews = p_reviews[[\"city\", \"date\", \"categories\", \"text\", \"stars_y\", \"state\"]] #stars_y because this is each individual\n",
    "p_reviews['year'] = pd.DatetimeIndex(p_reviews['date']).year\n",
    "p_reviews['month'] = pd.DatetimeIndex(p_reviews['date']).month\n",
    "p_reviews = p_reviews.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpus Summary\n",
    "* 2017 Yelp Academic Dataset\n",
    "* 4 million reviews about 144 thousand businesses\n",
    "* Pulled raw text, date, location, category, and star-ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Corpus\n",
    "\n",
    "In this notebook we will be looking at data taken from the 2017 Yelp Academic Dataset. The dataset is made publically available in the form of 6 JSON files. For our corpus, we converted two of these files (businesses and reviews) to csv files, and extracted only the required information for our analyses to better manipulate the data. Our corpus contains roughly 4 million reviews written by 1 million users about 144 thousand businesses. We decided to use the raw text of these reviews in combination with the location of businesses and when the review was written. In this notebook we will be exploring the linguistic differences between cities in relation to when the reviews are posted. One important feature of our corpus is that the data is not distributed uniformly meaning that there are a limited amount of reviews for some cities and many reviews for other cities. Although the data does not represent the entirety of the U.S., the data does give us a full range of latitudes stretching from the south of the U.S. up through Canada.More specifically, our corpus had a good amount of data for cities in the southwest, midwest, and the southeastern part of Canada. Therefore, we decided to focus on these areas and look for differences in the overall experiences of reviewers as well as their sentiments towards local businesses. In an effort to narrow our focus, we limited our data to reviews written in 2014, 2015, and 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "##Summary Stats\n",
    "print(\"Most common cities\")\n",
    "print(df['city'].value_counts()[:10])\n",
    "print('\\n')\n",
    "print(\"Most common states\")\n",
    "print(df['state'].value_counts()[:10])\n",
    "print('\\n')\n",
    "print(\"Scores distribution\")\n",
    "print(df['stars_x'].value_counts()/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exploratory Lessons\n",
    "\n",
    "The most important thing here is that the data is not distributed uniformly. There are only 6 U.S. states that have over 50,000 reviews: Nevada, Arizona, North Carolina, Ohio, Pennsylvania, and Wisconsin. In fact, most of the data is concentrated in only a few cities with Las Vegas, Phoenix, and Toronto topping the list. Although we do not have the entirety of the U.S. represented, the data does give us a full range of latitudes stretching from the south of the U.S. up through Canada.\n",
    "\n",
    "We also see that the reviews are disproportionately positive. Over 40% of all reviews are 5 stars, while stars 1,2, and 3 all have relatively equal rates of occurence.\n",
    "\n",
    "Now let's see if there's any obvious variation in that score distribution amongst some cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score distribution for Phoenix\")\n",
    "print(((df[df['city']==\"Phoenix\"]['stars_x'].value_counts())/len(df[df['city']==\"Phoenix\"])))\n",
    "print('\\n')\n",
    "print(\"Score distribution for Las Vegas\")\n",
    "print(((df[df['city']==\"Las Vegas\"]['stars_x'].value_counts())/len(df[df['city']==\"Las Vegas\"])))\n",
    "print('\\n')\n",
    "print(\"Score distribution for Pittsburgh\")\n",
    "print(((df[df['city']==\"Pittsburgh\"]['stars_x'].value_counts())/len(df[df['city']==\"Pittsburgh\"])))\n",
    "print('\\n')\n",
    "print(\"Score distribution for Madison\")\n",
    "print(((df[df['city']==\"Madison\"]['stars_x'].value_counts())/len(df[df['city']==\"Madison\"])))\n",
    "print('\\n')\n",
    "print(\"Score distribution for Toronto\")\n",
    "print(((df[df['city']==\"Toronto\"]['stars_x'].value_counts())/len(df[df['city']==\"Toronto\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploratory Lesson: Score Variation\n",
    "\n",
    "* Cities closer to each other have more similar frequencies of star ratings, and the further north you go, the lower the rate of 5 star reviews.\n",
    "    * Toronto: 29% 5 stars, 31% 4 stars\n",
    "    * Phoenix: 46% 5 stars, 22% 4 stars\n",
    "\n",
    "* So the score distribution changes as one moves from region to region; how about the actual language?\n",
    "\n",
    "* We used Vector Spaces to find out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Exploration: Vector Space Distances\n",
    "1) Narrow our focus down to a particular set of cities. \n",
    "\n",
    "2) Clean text, remove unnecessary characters, numbers, etc.\n",
    "\n",
    "3) Remove city names and other geographical proper nouns\n",
    "\n",
    "4) Random sample of 15,000 for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cities = [\"Las Vegas\", \"Phoenix\", \"Toronto\", \"Scottsdale\", \"Charlotte\", \"Pittsburgh\", \"Tempe\", \"Henderson\", \n",
    "\"Cleveland\", \"Madison\", \"Gilbert\", \"Mississauga\"]\n",
    "states = [df[df['city']==city]['state'].mode()[0]for city in cities]\n",
    "df[df['city']==city].sample(frac=1)\n",
    "n_cities = len(cities)\n",
    "text_df = pd.DataFrame(np.zeros((n_cities,2)), columns = [\"city\",\"raw_text\"])\n",
    "text_df['city'] = cities\n",
    "\n",
    "train_size = 15000 \n",
    "\n",
    "for i in range(0,len(text_df)):\n",
    "    city = text_df.iloc[i,0]\n",
    "    indiv_reviews = df[df['city']==city].sample(frac=1)\n",
    "    size = min(len(indiv_reviews),train_size)\n",
    "    raw_text = indiv_reviews['text'][:train_size].tolist()\n",
    "    raw_text = ' '.join(raw_text)\n",
    "    #preprocessing,,\n",
    "    raw_text = raw_text.lower()\n",
    "    raw_text = re.sub(\"\\n\",\"\",raw_text)\n",
    "    raw_text = re.sub(\"[^\\w\\s]\",\"\",raw_text)\n",
    "    raw_text = re.sub(\"\\d\",\"\",raw_text)\n",
    "    raw_text = re.sub(city.lower(), \"\", raw_text)\n",
    "    text_df.iloc[i,1] = raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "text = text_df['raw_text']\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "dtm = vectorizer.fit_transform(text)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "cos_dist = 1-cosine_similarity(dtm)\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(cos_dist)\n",
    "\n",
    "xs,ys = pos[:,0], pos[:,1]\n",
    "\n",
    "for x,y,name in zip(xs, ys, cities):\n",
    "    plt.scatter(x,y)\n",
    "    plt.text(x,y,name) \n",
    "\n",
    "linkage_matrix = ward(cos_dist)\n",
    "dendrogram(linkage_matrix, orientation=\"right\", labels=cities)\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://i.imgur.com/c1lAJQr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Standardizing Model  \n",
    "\n",
    "Before we rush to conclusions from that vector space, it is important to step back and take into consideration that our cities are not uniformly constructed. In other words, different cities are not made up of the same frequencies of businesses. Phoenix has more Mexican food than Toronto does. So it is important that we ensure this bias is not driving our variation.\n",
    "\n",
    "To do this, we narrowed our corpus down to a specific genre of business: nightlife. We filtered so that only businesses with the nightlife tag would be considered. These businesses are bars, clubs, or indoor activities like bowling. Controlling for one type of businesses helps us minimize the effect of business-type bias. When we do such an analysis we see a similar, albeit weaker, pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "split_cats = df['categories'].str.split(',',5,expand=True)\n",
    "df['first_cat'] = split_cats[0]\n",
    "df['second_cat'] = split_cats[1]\n",
    "df['third_cat'] = split_cats[2]\n",
    "df['fourth_cat'] = split_cats[3]\n",
    "first = df[df['first_cat']== \"['Nightlife'\"]\n",
    "second = df[df['second_cat']== \" 'Nightlife'\"]\n",
    "third = df[df['second_cat']==\" 'Nightlife']\"]\n",
    "Nightlife = pd.concat([first,second,third])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Image(url='http://i.imgur.com/TydCKRp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now that we have ensured our vector space is not entirely dominated by differences in business type, we return to our original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Original Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://i.imgur.com/c1lAJQr.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Preliminary Analysis\n",
    "\n",
    "We see a clear latitudinal trend in this graph. Southern cities are clustered at the top while northern cities are at the bottom. In fact, it appears that the linguistic similarity of two cities is directly proportional to the geographic distance between them. Starting with one city and ranking its similarity to the rest is essentially indistinguishable from ranking by lattitude. This was supported by a dendrogram analysis; clustering by physical distances produces almost the exact same results as clustering by language.\n",
    "\n",
    "We also notice that Canadian cities (Toronto and Mississauga) are particularly distinct from the other clusters, but still follow our pattern of being more similar to northern cities than southern ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question: What drives the linguistic distance between cities? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approach 1: Distinctive language\n",
    "\n",
    "**using supervised machine learning**\n",
    "\n",
    "1) Predicting Region\n",
    "* Distinctive words for each region\n",
    "\n",
    "2) Predicting Rating\n",
    "* Distinctive words for 1/5 star ratings faceted by region\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approach 2: Climatic differences\n",
    "\n",
    "**Relationship between weather/season and language**\n",
    "    \n",
    "1) Distinctive seasonal language\n",
    "    \n",
    "2) Positive sentiment in relation to season and  temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Method 1) Distinctive Language\n",
    "    A) Create a supervised machine learning model to use a review text as input and which predicts the region in which it was written. We can then look at the most distinctive words for each category (region)\n",
    "    \n",
    "    B) Divide the comments up into 3 regions, and train a model for each one. Then find what is the most distinctive words in each model for high stars and for low stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Preparing Data for machine learning\n",
    "* Add region feature\n",
    "* Filter by city\n",
    "* Remove punctuation and intereference words such as name of city and states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_size = 100000\n",
    "test_size = 25000\n",
    "\n",
    "midwest = [\"Cleveland\", \"Pittsburgh\", \"Madison\"]\n",
    "canada = [\"Toronto\", \"Mississauga\"]\n",
    "southwest = [\"Tempe\", \"Phoenix\", \"Scottsdale\", \"Gilbert\", \"Henderson\"]\n",
    "cities = midwest+canada+southwest\n",
    "regions = pd.DataFrame(cities, columns = ['city'])\n",
    "regions['region'] = ''\n",
    "for i in range(0, len(regions)):\n",
    "    city = regions.iloc[i,0]\n",
    "    if city in southwest:\n",
    "        regions.iloc[i,1] = \"Southwest\"\n",
    "    elif city in midwest:\n",
    "        regions.iloc[i,1] = \"Midwest\"\n",
    "    else:\n",
    "        regions.iloc[i,1] = \"Canada\"\n",
    "\n",
    "filtered_df = pd.merge(df, regions, on = \"city\")[['city', 'region', 'text', 'stars_x', 'date']]\n",
    "\n",
    "##Randomizing, moving the text to lowercase, removing the name of cities\n",
    "randomized = filtered_df.sample(frac=1, random_state=1)\n",
    "our_df = randomized[:(train_size+test_size)]\n",
    "\n",
    "all_text = (' this is a new string transition ').join(our_df.text)\n",
    "all_text = all_text.lower()\n",
    "\n",
    "for city in cities:\n",
    "    all_text = re.sub(city.lower(), \"\", all_text)\n",
    "\n",
    "all_text = re.sub(\"\\n\",\"\",all_text)\n",
    "all_text = re.sub(\"[^\\w\\s]\",\"\",all_text)\n",
    "all_text = re.sub(\"vour\", \"vor\", all_text)\n",
    "all_text = re.sub(\"arizona\", \"\", all_text)\n",
    "all_text = re.sub(\"az\", \"\", all_text)\n",
    "all_text = re.sub(\"\\d\",\"\",all_text)\n",
    "\n",
    "list_text = all_text.split(' this is a new string transition ')\n",
    "our_df['text'] = list_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Method 1A) Predicting Region\n",
    "\n",
    "This method will produce the word's most correlated with each city. By constructing a predictive model, we assign weights to each word in our corpus corresponding to how strongly that word is attached to each of our regions. We can then look at these relative weights to compare the most distinctive language of each region. These words may reveal two things. One, we will be able to see the exact words most connected to each region, which will give us a proxy of the vocabulary and unique lexicon of each region. Second, if we can identify patterns in these distinctive words we may be able to characterize these regions by what they hold most important, or at least what they talk about the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 1A) Predicting Region\n",
    "\n",
    "* Produces word's most correlated with each city\n",
    "* Aiming to reveal two things:\n",
    "    * See exact words most connected to each region, a proxy of the unique lexicon\n",
    "    * Identifying patterns in these words may help us characterize regions by what they deem most important\n",
    "        * At the least it will tell us what they talk about the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "our_df = our_df.sample(frac=1, random_state=1)\n",
    "train_df = our_df[:train_size]\n",
    "test_df = our_df[train_size:]\n",
    "tfidf_vec = TfidfVectorizer(stop_words = \"english\", min_df = .01, max_df = .95, binary = True)\n",
    "nb = MultinomialNB()\n",
    "train_dtm = tfidf_vec.fit_transform(train_df.text)\n",
    "test_dtm = tfidf_vec.transform(test_df.text)\n",
    "training_regions = train_df.region\n",
    "test_regions = test_df.region\n",
    "nb.fit(train_dtm, training_regions)\n",
    "predictitions_nb = nb.predict(test_dtm)\n",
    "print(accuracy_score(predictitions_nb, test_regions))\n",
    "def most_informative_features(classes, vectorizer = tfidf_vec, classifier = nb, top_n = 200):\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    class_index = np.where(classifier.classes_==(classes[0]))[0][0]\n",
    "    test_index1 = np.where(classifier.classes_==(classes[1]))[0][0]\n",
    "    test_index2 = np.where(classifier.classes_==(classes[2]))[0][0]\n",
    "    \n",
    "    class_prob_distro = np.exp(classifier.feature_log_prob_[class_index])\n",
    "    alt_class_prob_distro = np.exp(classifier.feature_log_prob_[test_index1]) + np.exp(classifier.feature_log_prob_[test_index2])\n",
    "    \n",
    "    odds_ratios = class_prob_distro / alt_class_prob_distro\n",
    "    odds_with_fns = sorted(zip(odds_ratios, feature_names), reverse = True)\n",
    "    \n",
    "    return odds_with_fns[:top_n]\n",
    "\n",
    "mw_features = most_informative_features(['Midwest', 'Southwest', 'Canada'])\n",
    "can_features = most_informative_features(['Canada', 'Southwest', 'Midwest'])\n",
    "sw_features = most_informative_features(['Southwest', 'Canada', 'Midwest'])\n",
    "\n",
    "##Sharing top words?\n",
    "mw_words = range(0,len(mw_features))\n",
    "for i in range(0,len(mw_features)):\n",
    "    mw_words[i] = mw_features[i][1]\n",
    "\n",
    "can_words = range(0,len(can_features))\n",
    "for i in range(0,len(can_features)):\n",
    "    can_words[i] = can_features[i][1]\n",
    "\n",
    "sw_words = range(0,len(sw_features))\n",
    "for i in range(0,len(sw_features)):\n",
    "    sw_words[i] = sw_features[i][1]\n",
    "\n",
    "mw_sw = [word for word in mw_words if word in sw_words]\n",
    "mw_can = [word for word in mw_words if word in can_words]\n",
    "can_sw = [word for word in can_words if word in sw_words]\n",
    "\n",
    "mw_dist = pd.DataFrame(mw_features, columns = ['Weight', 'Midwest_Top_Words'])\n",
    "can_dist = pd.DataFrame(can_features, columns = ['Weight', 'Canada_Top_Words'])\n",
    "sw_dist = pd.DataFrame(sw_features, columns = ['Weight', 'Southwest_Top_Words'])\n",
    "distinct_words = pd.concat([mw_dist, can_dist, sw_dist], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "distinct_words.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shared Midwest/Southwest:\", len(mw_sw))\n",
    "print(\"Shared Midwest/Canada:\", len(mw_can))\n",
    "print(\"Shared Canada/Southwest:\", len(can_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analysis\n",
    "\n",
    "Some of the most distinctive words are food groups. \n",
    "* Midwest: beers, bars, and cheese\n",
    "* Canada: fish and curry\n",
    "\n",
    "These reflect regional diets and interfere with true linguistic differences. Future analysis may want to account for this by controlling by genre (as we did with nightlife) or by removing food terms from the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are also more insightful interpretations. These generated words represent the most distinctive ways in which different cities comment on satisfaction, and they can be grouped into archetypes. For example:\n",
    "    * Expense: value, cost, price\n",
    "    * Quality: taste, portion, size\n",
    "    * Experience: service, professionalism, polite\n",
    "\n",
    "\n",
    "Categorizing our words into these groups, it appears that:\n",
    "    * the Southwest most cares about service\n",
    "    * the Midwest most about taste, \n",
    "    * and Canada a middle ground, with a higher emphasis on cost than either of the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we looked at shared words amongst the top 200:\n",
    "\n",
    "* Southwest and Midwest share 19\n",
    "* Midwest and Canada share 32\n",
    "* Southwest and Canada share 3\n",
    "\n",
    "This goes along with our theory that language becomes increasingly different as distance grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Method 1B) Predicting Rating\n",
    "\n",
    "This method will train a model to predict star rating for comments from each region. We can then see the words most correlated with high scores and low scores in each regions. These words will represent the language each region uses to talk about their satisfaction. We then filter out common words and create a list for each region of unique words most correlated with high and low satisfaction. Similar to the last analysis, we can use these words to establish a linguistic identity for each region; it will allow us to see the vocabulary of satisfaction for each region. We can then attempt to identify patterns and groups in those lists. Whereas the last method told us which words were most correlated with which regions, this method will tell us how each region uniquely talks about satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 1B) Predicting Rating\n",
    "* Will show us each region's words most correlated with high scores or low scores\n",
    "    * Gives insight into how each region talks about satisfaction\n",
    "* Filter out shared words to get a unique word list\n",
    "* Allows us to establish simple vocabularies (unique adjectives)\n",
    "* More importantly: we can identify patterns and groups to establish cultural themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Steps:\n",
    "* Filter to include only 1-star and 5-star reviews\n",
    "* Create separate data frames for each region\n",
    "* For each dataframe, train a model to predict star rating\n",
    "* Define distinct word function to print out 200 highest weighted words for each rating\n",
    "* Establish unique words for each region for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "stars_filtered_df = filtered_df[ (filtered_df['stars_x']==1) | (filtered_df['stars_x']==5)]\n",
    "southwest_df = stars_filtered_df[stars_filtered_df['region']==\"Southwest\"]\n",
    "midwest_df = stars_filtered_df[stars_filtered_df['region']=='Midwest']\n",
    "canada_df = stars_filtered_df[stars_filtered_df['region']=='Canada']\n",
    "\n",
    "max_sample = min(len(southwest_df), len(midwest_df), len(canada_df))\n",
    "train_size = int(.75*max_sample)\n",
    "test_size = int(.25*max_sample)\n",
    "\n",
    "##Southwest\n",
    "sw_df = southwest_df.sample(frac=1, random_state = 1)[:max_sample]\n",
    "\n",
    "sw_text = (' this is a new string transition ').join(sw_df.text)\n",
    "sw_text = sw_text.lower()\n",
    "    ##Preprocessing\n",
    "for city in cities:\n",
    "    sw_text = re.sub(city.lower(), \"\", sw_text)\n",
    "sw_text = re.sub(\"\\n\",\"\",sw_text)\n",
    "sw_text = re.sub(\"[^\\w\\s]\",\"\",sw_text)\n",
    "sw_text = re.sub(\"\\d\",\"\",sw_text)\n",
    "list_text = sw_text.split(' this is a new string transition ')\n",
    "sw_df['text'] = list_text\n",
    "    #Training\n",
    "sw_train_df = sw_df[:train_size]\n",
    "sw_test_df = sw_df[train_size:]\n",
    "sw_tfidf_vec = TfidfVectorizer(stop_words = 'english', min_df = .005, max_df = .8, binary = True)\n",
    "sw_nb = MultinomialNB()\n",
    "sw_train_dtm = sw_tfidf_vec.fit_transform(sw_train_df['text'])\n",
    "sw_test_dtm = sw_tfidf_vec.transform(sw_test_df['text'])\n",
    "sw_training_stars = list(sw_train_df.stars_x)\n",
    "sw_test_stars = list(sw_test_df.stars_x)\n",
    "sw_nb.fit(sw_train_dtm, sw_training_stars)\n",
    "sw_predictions_nb = sw_nb.predict(sw_test_dtm)\n",
    "\n",
    "##Midwest:\n",
    "mw_df = midwest_df.sample(frac=1, random_state=1)[:max_sample]\n",
    "mw_text = (' this is a new string transition ').join(mw_df.text)\n",
    "mw_text = mw_text.lower()\n",
    "    #Preprocessing\n",
    "for city in midwest:\n",
    "    mw_text = re.sub(city.lower(), \"\", mw_text)\n",
    "mw_text = re.sub(\"\\n\",\"\",mw_text)\n",
    "mw_text = re.sub(\"[^\\w\\s]\",\"\",mw_text)\n",
    "mw_text = re.sub(\"\\d\",\"\",mw_text)\n",
    "list_text = mw_text.split(' this is a new string transition ')\n",
    "mw_df['text'] = list_text\n",
    "    #Training\n",
    "mw_train_df = mw_df[:train_size]\n",
    "mw_test_df = mw_df[train_size:]\n",
    "mw_tfidf_vec = TfidfVectorizer(stop_words = 'english', min_df = .005, max_df = .8, binary = True)\n",
    "mw_nb = MultinomialNB()\n",
    "mw_train_dtm = mw_tfidf_vec.fit_transform(mw_train_df['text'])\n",
    "mw_test_dtm = mw_tfidf_vec.transform(mw_test_df['text'])\n",
    "mw_training_stars = list(mw_train_df.stars_x)\n",
    "mw_test_stars = list(mw_test_df.stars_x)\n",
    "mw_nb.fit(mw_train_dtm, mw_training_stars)\n",
    "mw_predictions_nb = mw_nb.predict(mw_test_dtm)\n",
    "\n",
    "##Canada:\n",
    "can_df = canada_df.sample(frac=1, random_state=1)[:max_sample]\n",
    "can_text = (' this is a new string transition ').join(can_df.text)\n",
    "can_text = can_text.lower()\n",
    "    ##Preprocessing\n",
    "for city in canada:\n",
    "    can_text = re.sub(city.lower(), \"\", can_text)\n",
    "can_text = re.sub(\"\\n\",\"\",can_text)\n",
    "can_text = re.sub(\"vour\", \"vor\", can_text)\n",
    "can_text = re.sub(\"[^\\w\\s]\",\"\",can_text)\n",
    "can_text = re.sub(\"\\d\",\"\",can_text)\n",
    "list_text = can_text.split(' this is a new string transition ')\n",
    "can_df['text'] = list_text\n",
    "    #Training\n",
    "can_train_df = can_df[:train_size]\n",
    "can_test_df = can_df[train_size:]\n",
    "can_tfidf_vec = TfidfVectorizer(stop_words = 'english', min_df = .005, max_df = .8, binary = True)\n",
    "can_nb = MultinomialNB()\n",
    "can_train_dtm = can_tfidf_vec.fit_transform(can_train_df['text'])\n",
    "can_test_dtm = can_tfidf_vec.transform(can_test_df['text'])\n",
    "can_training_stars = list(can_train_df.stars_x)\n",
    "can_test_stars = list(can_test_df.stars_x)\n",
    "can_nb.fit(can_train_dtm, can_training_stars)\n",
    "can_predictions_nb = can_nb.predict(can_test_dtm)\n",
    "\n",
    "print(\"Canada Accuracy Score:\", accuracy_score(can_predictions_nb,can_test_stars))\n",
    "print(\"Southwest Accuracy Score:\", accuracy_score(sw_predictions_nb,sw_test_stars))\n",
    "print(\"Midwest Accuracy Score:\", accuracy_score(mw_predictions_nb,mw_test_stars))\n",
    "\n",
    "def score_most_informative_features(star_no, vectorizer, classifier, bottom_n =0, top_n = 20):\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    class_index = np.where(classifier.classes_==(star_no))[0][0]\n",
    "    \n",
    "    class_prob_distro = np.exp(classifier.feature_log_prob_[class_index])\n",
    "    alt_class_prob_distro = np.exp(classifier.feature_log_prob_[1 - class_index])\n",
    "    \n",
    "    odds_ratios = class_prob_distro / alt_class_prob_distro\n",
    "    odds_with_fns = sorted(zip(odds_ratios, feature_names), reverse = True)\n",
    "    \n",
    "    return odds_with_fns[bottom_n:top_n]\n",
    "\n",
    "pos_midwest_words = score_most_informative_features(5, mw_tfidf_vec, mw_nb, bottom_n = 0, top_n = 200)\n",
    "mw_pos_top = []\n",
    "for i in range(0,99):\n",
    "    word = pos_midwest_words[i][1]\n",
    "    mw_pos_top.append(word)\n",
    "\n",
    "pos_southwest_words = score_most_informative_features(5, sw_tfidf_vec, sw_nb, bottom_n = 0, top_n = 200)\n",
    "sw_pos_top = []\n",
    "for i in range(0,99):\n",
    "    word = pos_southwest_words[i][1]\n",
    "    sw_pos_top.append(word)\n",
    "\n",
    "pos_canada_words = score_most_informative_features(5, can_tfidf_vec, can_nb, bottom_n = 0, top_n = 200)\n",
    "can_pos_top = []\n",
    "for i in range(0,99):\n",
    "    word = pos_canada_words[i][1]\n",
    "    can_pos_top.append(word)\n",
    "print()\n",
    "#negative words\n",
    "neg_midwest_words = score_most_informative_features(1, mw_tfidf_vec, mw_nb, bottom_n = 0, top_n = 200)\n",
    "mw_neg_top = []\n",
    "for i in range(0,99):\n",
    "    word = neg_midwest_words[i][1]\n",
    "    mw_neg_top.append(word)\n",
    "\n",
    "neg_southwest_words = score_most_informative_features(1, sw_tfidf_vec, sw_nb, bottom_n = 0, top_n = 200)\n",
    "sw_neg_top = []\n",
    "for i in range(0,99):\n",
    "    word = neg_southwest_words[i][1]\n",
    "    sw_neg_top.append(word)\n",
    "\n",
    "neg_canada_words = score_most_informative_features(1, can_tfidf_vec, can_nb, bottom_n = 0, top_n = 200)\n",
    "can_neg_top = []\n",
    "for i in range(0,99):\n",
    "    word = neg_canada_words[i][1]\n",
    "    can_neg_top.append(word)\n",
    "\n",
    "can_pos_unique = [word for word in can_pos_top if word not in sw_pos_top]\n",
    "can_pos_unique = [word for word in can_pos_unique if word not in mw_pos_top]\n",
    "mw_pos_unique = [word for word in mw_pos_top if word not in can_pos_top]\n",
    "mw_pos_unique = [word for word in mw_pos_unique if word not in sw_pos_top]\n",
    "sw_pos_unique = [word for word in sw_pos_top if word not in can_pos_top]\n",
    "sw_pos_unique = [word for word in sw_pos_unique if word not in mw_pos_top]\n",
    "can_neg_unique = [word for word in can_neg_top if word not in sw_neg_top]\n",
    "can_neg_unique = [word for word in can_neg_unique if word not in mw_neg_top]\n",
    "mw_neg_unique = [word for word in mw_neg_top if word not in can_neg_top]\n",
    "mw_neg_unique = [word for word in mw_neg_unique if word not in sw_neg_top]\n",
    "sw_neg_unique = [word for word in sw_neg_top if word not in can_neg_top]\n",
    "sw_neg_unique = [word for word in sw_neg_unique if word not in mw_neg_top]\n",
    "can_unique = can_pos_unique+can_neg_unique\n",
    "mw_unique = mw_pos_unique+mw_neg_unique\n",
    "sw_unique = sw_pos_unique+sw_neg_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Canada's Unique Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(can_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Notice a predilection to the unique:\n",
    "* hidden\n",
    "* secret\n",
    "* unlike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Midwest's Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(mw_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Appears Midwest has a sweet tooth\n",
    "\n",
    "* caramel, pastries, crepes\n",
    "* frites, popcorn, cinnamon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Southwest Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sw_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Service and Experience\n",
    "* Inviting\n",
    "* Caring\n",
    "* Courteous\n",
    "* Easy\n",
    "* Satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Analysis:\n",
    "\n",
    "Different regions use distinct language to describe their satisfaction. This manifests as vocabulary, e.g. the Midwest heavily uses terrific and delightful while other regions don't. Distinct language also seems to reveal patterns of satisfaction. We see a Canadian tendency to enjoy the unique and undiscovered. Midwesterners love their desserts. Southwesterners care deeply about service and experience. Notice how congruent these results are with our original analysis where we found that the Midwest focused on taste and the Southwest on service.\n",
    "\n",
    "There are some important notes to keep in mind. First, the reviews were aggregated by region, rather than city. This method can be very easily adapted to instead compare individual cities rather than groups. This approach would not be ideal for answering our specific question (since we observed linguistic grouping of regions), but would be helpful for other questions that wanted to more deeply explore a city's linguistic identity. Furthermore, while a random sample avoids bias in the method, there is inherent bias in the make up of our data. In other words, different cities and regions have different frequencies of types of businesses. Cities with more of one type of restaurant or business will show distinct words bent towards those genres. This is somewhat balanced out by the fact that a higher frequency of certain types of business is also a part of the city's identity. Nonetheless, access to a larger corpus would allow us to re-conduct this study and control for specific genres of businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analysis Summary:\n",
    "* Differences in basic vocabulary\n",
    "    * Midwest uniquely uses \"terrific\" and \"delightful\", Southwest uses \"informative\", Canada \"pleasantly\".\n",
    "    * With a sample our size these are likely not due to random sampling error\n",
    "    * Supports idea that these methods can be used to establish a linguistic identity \n",
    "* Thematic Differences:\n",
    "    * Canadian tendency towards the unique and undiscovered: \"hidden\", \"secret\", \"unlike\"\n",
    "    * Midwestern predilection towards taste and dessert: \"carmel\", \"pastries\", \"cookies\", etc.\n",
    "    * Southwest appreciates service: \"inviting\", \"caring\", \"courteous\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approach 2: Climatic and Seasonal differences between cities drive linguistic distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Seasonal  TF-IDF Scores\n",
    "\n",
    "Here we used term frequencyinverse document frequency scores to obtain a numerical statistic that is intended to reflect how important a word is to a review given all the other reviews. By getting the words scores, which weigh words by their frequency in one review compared to their distribution across all reviews, we can get an indication of the content of that review and filter out common stop words such as 'the', 'of', and 'and'. We will then split these scores up by season in order to check if there is a visible difference in topic by season and by each city. The way we split up seasons is as follows:\n",
    "April-June is Spring, July-September is Summer, October-December is Fall, January-March is Winter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Seasonal TF-IDF scores\n",
    "* TF-IDF produces a numerical statistic intended to reflect how important a word is to a review given all other reviews. It weights words by their frequency in one review compared to their distribution across the rest.\n",
    "* This allows us to filter out words that are not actually meaningful or distinct\n",
    "* We grouped by season and conducted this test to see if there was a noticeable difference in topic by season in each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#randomize\n",
    "samp = p_reviews.sample(1045151, random_state=0)\n",
    "\n",
    "#take a sample\n",
    "sample = samp[:10000]\n",
    "\n",
    "#removing digits\n",
    "sample['text'] = sample['text'].apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #removing digits\n",
    "\n",
    "#removing proper nouns and foreign words \n",
    "sample['text_tokens'] = sample['text'].apply(nltk.word_tokenize)\n",
    "sample['text_tags'] = sample['text_tokens'].apply(nltk.pos_tag)\n",
    "sample['textt'] = sample['text_tags'].apply(lambda x: ''.join([word + \" \" for word, pos in x if pos != 'NNP'and pos != 'NNPS'and pos != 'FW']))         \n",
    "\n",
    "#Show distribution of reviews by month in 2016\n",
    "sample_stars = sample.groupby(\"month\")\n",
    "sample_stars['text'].count().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#exploring the data to make sure there are enough reviews in each season (same as above but grouped by season)\n",
    "Spring = sample[(sample['month'] == 5) | (sample['month'] == 4) | (sample['month'] == 6)]\n",
    "Summer = sample[(sample['month'] == 7) | (sample['month'] == 8) | (sample['month'] == 9)]\n",
    "Fall = sample[(sample['month'] == 11) | (sample['month'] == 10) | (sample['month'] == 12)]\n",
    "Winter = sample[(sample['month'] == 1) | (sample['month'] == 2) | (sample['month'] == 3)]\n",
    "print(\"Spring: \")\n",
    "print(Spring.month.value_counts())\n",
    "print(\"Fall: \")\n",
    "print(Fall.month.value_counts())\n",
    "print(\"Summer: \")\n",
    "print(Summer.month.value_counts())\n",
    "print(\"Winter: \")\n",
    "print(Winter.month.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#justification of which cities to use \n",
    "#used these cities to represent midwest, canada, and southwest because they have similar amount of reviews giving better results\n",
    "#(some cities had no reviews for certain months making the functions not work)\n",
    "pit = sample[sample['city'].str.contains(\"Pittsburgh\")]\n",
    "tor = sample[sample['city'].str.match(\"Toronto\")]\n",
    "pho = sample[sample['city'].str.match(\"Phoenix\")]\n",
    "print(pit.month.value_counts())  #pittsburgh had more reviews than other midwest cities\n",
    "print(pho.month.value_counts())   #phoenix had more reviews than other southwest cities\n",
    "print(tor.month.value_counts())  #toronto had more reviews than other canadian cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pit = sample[sample['city'].str.contains(\"Pittsburgh\")]\n",
    "tor = sample[sample['city'].str.match(\"Toronto\")]\n",
    "pho = sample[sample['city'].str.match(\"Phoenix\")]\n",
    "\n",
    "def tfidf(city):\n",
    "    #create dataset with document index and month\n",
    "    sample_month = city['month'].to_frame() \n",
    "    \n",
    "    #merge this into the dtm_tfidf_df\n",
    "    tfidfvec = TfidfVectorizer()\n",
    "    dtm_tfidf_df = pd.DataFrame(tfidfvec.fit_transform(city.text).toarray(), columns=tfidfvec.get_feature_names(), index = city.index)\n",
    "    merged_df = sample_month.join(dtm_tfidf_df, how = 'right', lsuffix='_x') #suffix to defferentiate from column stars_y and word stars\n",
    "    \n",
    "    dtm_spring = merged_df[(merged_df['month_x'] == 5) | (merged_df['month'] == 4) | (merged_df['month'] == 6)]\n",
    "    dtm_summer = merged_df[(merged_df['month_x'] == 7) | (merged_df['month'] == 8) | (merged_df['month'] == 9)]\n",
    "    dtm_winter = merged_df[(merged_df['month_x'] == 1) | (merged_df['month'] == 2) | (merged_df['month'] == 3)]\n",
    "    dtm_fall = merged_df[(merged_df['month_x'] == 10) | (merged_df['month'] == 11) | (merged_df['month'] == 12)]\n",
    "\n",
    "    #print the words with the highest tf-idf scores for each month\n",
    "    spring = pd.DataFrame(dtm_spring.max().sort_values(ascending=False)[0:20].index, columns = [\"Spring\"]) #numeric_only=True \n",
    "    summer = pd.DataFrame(dtm_summer.max().sort_values(ascending=False)[0:20].index, columns = [\"Summer\"])\n",
    "    winter = pd.DataFrame(dtm_winter.max().sort_values(ascending=False)[0:20].index, columns = [\"Winter\"])\n",
    "    fall = pd.DataFrame(dtm_fall.max().sort_values(ascending=False)[0:20].index, columns = [\"Fall\"])\n",
    "    frame = pd.concat([spring, summer, winter, fall], axis = 1)\n",
    "    print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Phoenix \n",
    "tfidf(pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Pittsburgh\n",
    "tfidf(pit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Toronto\n",
    "tfidf(tor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Discussion of Results: \n",
    "\n",
    "    Overall, these results were not so fruitful in the sense that the words did not give us clear differences in seasons or indication that cities experience seasons differently when it comes to their reviews of local businesses. However, some words show what we would expect from the simulation. For example, the word \"doctor\" shows up in Toronto's winter reviews which may indicate an increase of doctor visits due to lower temperatures. We also noticed that seafood was exclusively mentioned in the Spring and Summer reviews accross all citites potentially indicating that most types of seafood can be caught or farmed locally during these months (which from what I researched seems to be true). Another observation was that in Phoenix, the common beverages were \"beergaritas\", coffee, and whiskey. In Pittsburgh  people drank juice, wine, beer, and coke. In Toronto, people seem to be drinking mainly beer and monsters (potentially refering to the kind of soda?). We also found it interesting that \"Portugeese\" shows up in Toronto's Spring reviews, \"italian\" in the Winder reviews, and \"taiwan\" in the Fall reviews. \n",
    "    After doing this analysis, we wanted to look deeper into customer experience. This brings us to our next analysis which looks more specidically at positive and negative sentiment analysis. We will look specifically at temperature differences and seasonal differences of specific cities in the midwest, the southwest, and canada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF Results Summary\n",
    "* Words did not show clear differences in seasonal vocabulary\n",
    "* Do see results that make sense\n",
    "    * Doctor shows up in winter reviews, perhaps due to an increase in doctor's visits in cold temperatures\n",
    "    * Seafood only mentioned during the Spring and Summer reviews\n",
    "* Again see regional diets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "* Recall that we observed a latitudinal trend of linguistic distance.\n",
    "\n",
    "* We hypothesize that this is partially due to a relationship between lattitude and positive sentiment in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "##Selecting all cities above a certain population\n",
    "desired_min_population = 5000\n",
    "top_cities = df['city'].value_counts()[df['city'].value_counts()>desired_min_population].index\n",
    "top_states = [df[df['city']==city]['state'].mode()[0]for city in top_cities]\n",
    "n_cities = len(top_cities)\n",
    "text_df = pd.DataFrame(np.zeros((n_cities,2)), columns = [\"city\",\"raw_text\"])\n",
    "text_df['city'] = top_cities\n",
    "\n",
    "#for each row (city) in the frame, collect a random sample of reviews, concat them, and put them next to the city\n",
    "\n",
    "for i in range(0,len(text_df)):\n",
    "    city = text_df.iloc[i,0]\n",
    "    indiv_reviews = df[df['city']==city].sample(frac=1)\n",
    "    size = min(len(indiv_reviews),train_size)\n",
    "    raw_text = indiv_reviews['text'][:train_size].tolist()\n",
    "    raw_text = ' '.join(raw_text)\n",
    "    #preprocessing\n",
    "    raw_text = raw_text.lower()\n",
    "    raw_text = re.sub(\"\\n\",\"\",raw_text)\n",
    "    raw_text = re.sub(\"[^\\w\\s]\",\"\",raw_text)\n",
    "    raw_text = re.sub(\"\\d\",\"\",raw_text)\n",
    "    raw_text = re.sub(city.lower(), \"\", raw_text)\n",
    "    text_df.iloc[i,1] = raw_text\n",
    "\n",
    "##using that df make a DTM where each row corresponds to a different city\n",
    "text = text_df['raw_text']\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "dtm = vectorizer.fit_transform(text)\n",
    "\n",
    "##create vocab dtm\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab_df = pd.DataFrame(dtm.toarray(), columns = vocab)\n",
    "vocab_df['word_count'] = vocab_df.sum(axis=1)\n",
    "\n",
    "pos_sent = open(\"../data/positive_words.txt\").read()\n",
    "neg_sent = open(\"../data/negative_words.txt\").read()\n",
    "positive = pos_sent.split('\\n')\n",
    "negative = neg_sent.split('\\n')\n",
    "\n",
    "#Make a pos_dtm and neg_dtm by keeping corresponding vocabulary.\n",
    "pos_vocab = [word for word in vocab if word in positive]\n",
    "pos_dtm = vocab_df[pos_vocab]\n",
    "pos_dtm['pos_count'] = pos_dtm.sum(axis=1)\n",
    "pos_dtm['word_count'] = vocab_df['word_count']\n",
    "pos_dtm['prop_pos'] = pos_dtm['pos_count']/vocab_df['word_count']\n",
    "neg_vocab = [word for word in vocab if word in negative]\n",
    "neg_dtm = vocab_df[neg_vocab]\n",
    "neg_dtm['neg_count'] = neg_dtm.sum(axis=1)\n",
    "neg_dtm['word_count'] = vocab_df['word_count']\n",
    "neg_dtm['prop_neg'] = neg_dtm['neg_count']/vocab_df['word_count']\n",
    "\n",
    "#make a dataframe with city, state, latitude, and longitude\n",
    "city_lats = pd.DataFrame(np.zeros((n_cities,3)), columns = ['city', 'latitude','longitude'])\n",
    "city_lats = pd.DataFrame(np.zeros((n_cities,3)), columns = ['city', 'latitude','longitude'])\n",
    "city_lats['city']=top_cities\n",
    "city_lats['state'] = top_states\n",
    "\n",
    "\n",
    "for i in range(0,n_cities):\n",
    "    city = top_cities[i]\n",
    "    lat = df[df['city']==city].latitude.mean()\n",
    "    lon = df[df['city']==city].longitude.mean()\n",
    "    city_lats.iloc[i, 1] = lat\n",
    "    city_lats.iloc[i, 2] = lon\n",
    "\n",
    "#Following graph made in R using data in city_lats and pos_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://i.imgur.com/CzUYUqU.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Fitting a linear model in R reveals that regressing the difference in pos/neg proportions on latitude is the best model, giving a very significant p statistic of .03\n",
    "\n",
    "So, northern cities use less positive language. \n",
    "\n",
    "Why is this  the case? People do not simply change their moods based off what number latitude they are on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seasonal Sentiment Analysis\n",
    "* Again we look towards the varying effects of season on cities using positive and negative sentiment and looking at three specific cities (Pittsburg, Phoenix, Toronto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#loading in the data\n",
    "pos_sent = open(\"./Data/positive_words.txt\").read()\n",
    "neg_sent = open(\"./Data/negative_words.txt\").read()\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "positive_words = pos_sent.split('\\n')\n",
    "negative_words = neg_sent.split('\\n')\n",
    "\n",
    "areas = [\"Phoenix\", \"Pittsburgh\", \"Toronto\"]\n",
    "mySeasonString = ['Fall', 'Spring', 'Summer', 'Winter']\n",
    "countvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#getting proportion of negative sentiment for each season in a city - test one of each city (phoenix pittsburgh toronto)\n",
    "def NegPropSeasons(seasons):\n",
    "    lst = []\n",
    "    for i in range(len(seasons)):\n",
    "        mySeason = seasons[i]\n",
    "        mySeason['text_tokens'] = mySeason['text'].apply(nltk.word_tokenize)\n",
    "        mySeason['text_tokens'] = mySeason['text_tokens'].apply(lambda x: [word for word in x if word not in punctuations])            \n",
    "        mySeason['token_count'] = mySeason['text_tokens'].apply(lambda x: len(x))\n",
    "        mySeason['num_pos_words'] = mySeason['text_tokens'].apply(lambda x: [word for word in x if word in positive_words]).apply(len)          \n",
    "        mySeason['prop_positive'] = mySeason['num_pos_words']/mySeason[\"token_count\"]\n",
    "        mySeason['num_neg_words'] = mySeason['text_tokens'].apply(lambda x: [word for word in x if word in negative_words]).apply(len)\n",
    "        mySeason['prop_negative'] = mySeason['num_neg_words']/mySeason[\"token_count\"]\n",
    "        dtm_df = pd.DataFrame(countvec.fit_transform(mySeason.text).toarray(), columns=countvec.get_feature_names(), index = mySeason.index)\n",
    "        columns = list(dtm_df)\n",
    "        neg_columns = [word for word in columns if word in negative_words]\n",
    "        dtm_neg = dtm_df[neg_columns]\n",
    "        dtm_neg['neg_count'] = dtm_neg.sum(axis=1)\n",
    "        dtm_neg['neg_proportion'] = dtm_neg['neg_count']/dtm_df.sum(axis=1)\n",
    "        dtm_neg['avg'] = dtm_neg['neg_proportion'].mean() #gives me average of the proportions of positive words in each review for each season\n",
    "        lst.append(dtm_neg['neg_proportion'].mean())\n",
    "        \n",
    "    print(\"Average Proportion of Negative words in Fall 2016:\")\n",
    "    print(lst[0]) \n",
    "    print(\"Average Proportion of Negative words in Spring 2016:\") \n",
    "    print(lst[1])\n",
    "    print(\"Average Proportion of Negative words in Summer 2016:\") \n",
    "    print(lst[2])\n",
    "    print(\"Average Proportion of Negative words in Winder 2016:\") \n",
    "    print(lst[3])\n",
    "    \n",
    "    \n",
    "    df2 = pd.DataFrame(index = range(1), columns=['Fall', 'Spring', 'Summer', 'Winter'])\n",
    "    df2.loc[0] = lst\n",
    "    ax = df2.plot.bar()\n",
    "    ax.set_xlabel('Seasons', fontsize=12)\n",
    "    ax.set_ylabel('Fequency of Negative Words', fontsize=12)\n",
    "    ax.set_ylim(.08, .089)\n",
    "    \n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.title(\"Average Proportion of Negative Words According to Season\")\n",
    "    plt.show()\n",
    "    \n",
    "def PosPropSeasons(seasons):\n",
    "    for i in range(len(seasons)):\n",
    "        mySeason = seasons[i]\n",
    "        mySeason['text_tokens'] = mySeason['text'].apply(nltk.word_tokenize)\n",
    "        mySeason['text_tokens'] = mySeason['text_tokens'].apply(lambda x: [word for word in x if word not in punctuations])\n",
    "        mySeason['token_count'] = mySeason['text_tokens'].apply(lambda x: len(x))\n",
    "        mySeason['num_pos_words'] = mySeason['text_tokens'].apply(lambda x: [word for word in x if word in positive_words]).apply(len)\n",
    "        mySeason['prop_positive'] = mySeason['num_pos_words']/mySeason[\"token_count\"]\n",
    "        mySeason['num_neg_words'] = mySeason['text_tokens'].apply(lambda x: [word for word in x if word in negative_words]).apply(len)\n",
    "        mySeason['prop_negative'] = mySeason['num_neg_words']/mySeason[\"token_count\"]\n",
    "        dtm_df = pd.DataFrame(countvec.fit_transform(mySeason.text).toarray(), columns=countvec.get_feature_names(), index = mySeason.index)\n",
    "        columns = list(dtm_df)\n",
    "        pos_columns = [word for word in columns if word in positive_words]\n",
    "        dtm_pos = dtm_df[pos_columns]\n",
    "        dtm_pos['pos_count'] = dtm_pos.sum(axis=1)\n",
    "        dtm_pos['pos_proportion'] = dtm_pos['pos_count']/dtm_df.sum(axis=1)\n",
    "        dtm_pos['avg'] = dtm_pos['pos_proportion'].mean() #gives me average of the proportions of positive words in each review for each season\n",
    "        dtm_pos.avg.plot.hist(alpha=0.5, bins = 1, label = mySeasonString[i])\n",
    "        #dtm_pos.pos_proportion.plot(stacked=True, kind = 'hist', bins = 5, )  #label = str(mySeason)   \n",
    "        #dtm_pos.pos_proportion.plot.area(stacked=False)\n",
    "            \n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.title(\"Looking at Positive Proportion According to Season\")\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#getting each season's historgram for each city - merge the histograms by city\n",
    "for i in range(len(areas)):\n",
    "    print(areas[i])\n",
    "    city = p_reviews[p_reviews['city'].str.contains(areas[i])]\n",
    "    Spring = city[(city['month'] == 4) | (city['month'] == 5) | (city['month'] == 6)]\n",
    "    Summer = city[(city['month'] == 7) | (city['month'] == 8) | (city['month'] == 9)]\n",
    "    Fall = city[(city['month'] == 10) | (city['month'] == 11) | (city['month'] == 12)]\n",
    "    Winter = city[(city['month'] == 1) | (city['month'] == 2) | (city['month'] == 3)]\n",
    "    \n",
    "    NegPropSeasons([Fall, Spring, Summer, Winter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Positive sentiment analysis showed more meaningful results than negative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Image(url = 'http://i.imgur.com/gQMcf1d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://i.imgur.com/SuQblzP.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://i.imgur.com/OnJTJtD.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Discussion of Results:\n",
    "    \n",
    "These results show that Phoenix has the highest proportion of positive words in the Fall followed by Spring, Winter, and Summer. Pittsburg has the the highest proportion of positive words in the Spring followed by Winter, Fall, and Summer. Lastly, Toronto shows the same pattern as Pittsburg (having more positive words come up in Spring and Winter than in the Summer) showing that cities that are in closer proximity to one another have similar results when measuring sentiment accross seasons. \n",
    "    \n",
    "We also looked at the averages of the proportion of negative words by season. From these bar graphs, it looks like Phoenix has the highest proportion of negative words in the Winter. The next highest is Summer followed by Spring and Fall. However, Pittsburgh's order of highest proportion of negative words is Spring, Summer, Fall, and then Winter. Toronto, once again shows the same pattern as Pittsburg with slightly different variance between seasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion Summary:\n",
    "* Expected northern cities to have less positive words in the Winter.\n",
    "* Phoenix has higher positive words in Fall and Winter than Pittsburg and Toronto actually had more positive words in Fall and Winter.\n",
    "    *Difference in weather expectations?\n",
    "    \n",
    "* Found that cities in close proximity to one another have similar positive/negative sentiment accorss seasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Investigating Temperature\n",
    "\n",
    "The most obvious difference between latitudes is temperature. So we pulled temperature data for a handful of our cities from the National Oceanic and Atmospheric Administration's Center for Environmental Information. \n",
    "\n",
    "We then matched the date of each review with the mean temperature of that day, and plotted the results for each city. Note: Negative sentiment did not reveal as meaningful results, more on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(url='http://i.imgur.com/kqSWom4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Positive Correlation\n",
    "\n",
    "P-Value=.18 (F-stat 2.696 on 1 and 45427 d.f.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(url='http://i.imgur.com/DcDWRLl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Positive Correlation\n",
    "\n",
    "P-Value=.1515 (F-stat 2.057 on 1 and 32983 d.f.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(url='http://i.imgur.com/GmoI9fP.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Positive Correlation\n",
    "\n",
    "P-Value=.312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(url='http://i.imgur.com/pyLV7Wu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "No Correlation\n",
    "\n",
    "P-Value=.9465 (F-stat .0045 on 1 and 31797 d.f.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temperature Analysis\n",
    "* Toronto's relationship is fairly significant (~.18) \n",
    "* Madison's relationship is fairly significant (~.15)\n",
    "* Charlotte's relationship is not statistically significant on its own (~.31)\n",
    "* Phoenix, shows no linear relationship (~.95)\n",
    "\n",
    "The further North one goes (e.g. the more one is affected by cold temperature) the stronger the correlation between temperature and positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Hypothesis 1: Different regions use different vocabulary to speak about satisfaction.\n",
    "\n",
    "Using machine learning to produce regionally distinctive words supported this hypothesis. We found that closer regions shared more distinct words. We also saw that while there was overlap in the words used to describe satisfaction, each region used certain adjectives at such higher levels that those words became uniquely distinct for their respective region. This is exactly the kind of results we hypothesized: certain seemingly ordinary words are uniquely common to particular regions. Aggregating these words can produce a vocabulary or identity for regions and cities. Culturally this means we should expand our understanding of dialects beyond simple slang or idiosyncracies. Midwest language is defined by much more than their use of the word pop, or the way they pronounce \"been\". Beyond that, they have a fundamentally different frequency use of the English language. Even with a sample as large as ours we see a bias of that region towards simple words like \"terrific\", whereas other regions show similar bias towards other seemingly innocuous adjectives. Why exactly these regions choose such adjectives is impossible to say from this analysis, and likely will find its answer in a non-computational approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, we saw that certain regions favored particular words that could be grouped into linguistic gestalts. The Midwest talked considerably more about taste, specifically desserts, while the Southwest cared more about service and experience. Canada stood in the middle ground but showed a strong tendency to enjoy hidden or unique venues. These results went beyond our hypothesis. It appears possible that cities not only use different words to describe satisfaction, but also weight factors differently when determining their level of satisfaction. Determining these weightings is an empirical way of constructing a city's cultural identity. It should be noted that these results could have been arrived at in two ways. The first is our method, using a data driven approach to determine what cultural themes are present in the language. The second would be a more social-science, hand driven approach where the cultural themes were first identified in the population, and that data would be used to guess at the language that would appear. Our method has allowed us to establish these trends without imposing the bias of a hand-driven approach. We arrived at these cultural themes from nothing more than text and numbers.\n",
    "\n",
    "This part of our study has shown that the language of a region, and especially its language of satisfaction, is influenced by at least two major factors. First, different regions have vocabulary differences that stretch beyond slang and into the very fabric of the words they choose. Second, language is heavily driven by cultural preferences, and these cultural preferences can be distilled from a data-driven analysis of distinct language. This knowledge can be utilized to either continue to analyze differences in language, or it can be used as a jumping off point to identify cultural trends and tendencies in different cities and regions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Hypothesis 2: Climate and Season affect language (and thus drive regional differences)\n",
    "\n",
    "    \n",
    "When we looked at sentiment analysis by season, one takeaway is that the bar graphs for positive word proportions shows a greater variance between seasons than the negative word proportions chart. This may be due to the fact that individuals voice their satisfaction more than their negative experiences. Though these outputs are somewhat different than what we expected (northern cities to have higher negative and lower positive words than southern cities), the data shows that cities in close proximity to one another have similar positive/negative trends when it comes to sentiment accross seasons. This brings us to our final graphs where we try to uncover how temperature affects our sentiment analysis results among different cities by looking at each specific day's temperature. \n",
    "\n",
    "\n",
    "After establishing a clear correlation between latitude and positive sentiment, we hypothesized that temperature affected positive language. Since temperature varied from city to city, this would explain some of the regional differences in language. We found strong evidence supporting this claim. The higher the latitude in the United States, the more cold temperature becomes a factor. Similarly, we found that northern cities (Madison and Toronto) showed a stronger relationship between temperature and positivity than did mid-line cities (Charlotte) while southern cities (Phoenix) showed absolutely no linear correlation. This leads us to conclude that cold weather affects positive sentiment in language, and since cold weather affects cities differently, this drives some of the linguistic difference we originally found. Thus, temperature should be interpreted as a confounding variable. Some of the linguistic distance that we discovered can be explained by the varying presence of cold temperatures, rather than any inherent cultural truths.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take Aways:\n",
    "\n",
    "Linguistic distance between cities is driven by at least 3 major factors:\n",
    "\n",
    "1) **Vocabulary**:\n",
    "    Regions use certain words at different rates (each region had their own unique adjectives).\n",
    "\n",
    "2) **Cultural Themes**: Regions have cultural themes of satisfaction that dominate their language in this corpus. \n",
    "    \n",
    "3) **Climatic Effects**: Season and temperature influence sentiment, and since they affect cities differently, this drives some of the linguistic distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improvements/Limitations\n",
    "\n",
    "* Data from more states\n",
    "* Control for temperature to investigate vocabulary only\n",
    "* More focused approach:\n",
    "    * Picking and justifying certain features to investigate\n",
    "* Optional: removing food, region specific nouns, proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Future Research\n",
    "\n",
    "* City by city study rather than region\n",
    "* Different corpus, perhaps Twitter or Glassdoor data for employment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Planned Division of Labor:\n",
    "\n",
    "The division of labor in our project was pretty easygoing. We both did a lot of preliminary data analysis to better understand our massive corpus and what we wanted to focus on for the sake of this project. We agreed on narrowing it down to looking specifically at location, lingistic differences, and seasonal differences to focus our project on one specific questions: What drives the differences in Yelp reviews among cities. While Landon did some statistical data analysis with geospacial location data using R to try to better understand linguistic differences in our chosen cities, Pauline used python to explore the differences in seasonal variation of positive and negative sentiment. We both created data visualizations to demonstate what we found in our separate analyses. After meeting a couple times to refine our topic and coordinate our analyses, we each wrote up parts of the final Report.\n",
    "\n",
    "### Splitting up the Presentation:\n",
    "\n",
    "Landon will describe the question or puzzle we are exploring, why it is interesting or important, how others have attempted to answer this question, and how we are improving on these answers. \n",
    "\n",
    "Pauline will describe the corpus we are using to explore the question and how you collected the corpus as well as the summary statistics of the corpus. (Counts and Vector Space)\n",
    "\n",
    "Landon and Pauline will discuss their analyses and describe one analysis process each and why it is appropriate for your question and text, followed by code implementing the techniques, output from the calculations, and a description of how we understand the output/visualizations.\n",
    "\n",
    "Landon: Our broader conclusions about history and the world around us with evidence from your analysis. \n",
    "\n",
    "Pauline: Further analyses and other texts that could help us continue to explore your question."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "height": 600,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif",
   "transition": "zoom",
   "width": 1280
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
